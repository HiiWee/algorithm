### [1.3 알고리즘의 분석]
<br>

* 어떤 알고리즘에 대한 두 가지 중요한 질문
  * 정확한가? : 모든 입력 사례에 대해서 해답을 찾을 수 있는가? (`correctness`)
  * 효율적인가? : 입력 크기(`n`)가 커지면 성능(`함수 f(x)`)이 어떻게 변화하는가?
<br><br>
* 알고리즘의 분석
  * 정확성 분석 : 모든 입력 사례에 대해서 정확한 해답을 찾는다는 것을 증명 (대표적으로 `수학적 귀납법`)
  * 효율성 분석 : 입력 크기가 커지는 정도에 따라 성능의 변화량을 증명
    * `시간 복잡도(time complexity)` : 시간을 기준으로 알고리즘의 효율성 분석 (주로 이용)
    * 공간 복잡도(space complexity) : 공간을 기준으로 알고리즘의 효율성 분석
<br><br>
* 알고리즘의 성능 분석
  *  퍼포먼스 측정 : 실행 시간을 직접 측정 or 실행 명령의 숫자 세기
     *  `한계` : 컴퓨터의 성능이나 프로그래밍 언어에 따라 달라짐
  * 복잡도 분석 : 컴퓨터나 프로그래밍 언엉와 `무관`하게 성능 분석
    * 입력 크기에 따른 단위 연산의 실행 횟수 세기
<br><br>
* 복잡도 분석
  * 입력 크기 : 문제가 가진 파라미터, 즉 입력 사례의 크기(n으로 둠)
  * 단위 연산(`Basic Operation`) : 알고리즘 실행의 기본이 되는 명령어들의 집합
  <br>
  <br>
  * Algorithm 1.2 (배열 원소의 합)의 시간 복잡도 분석
    ```python
      def sum(S):
        n = len(S)
        result = 0
        for i in ragne(n):
          result += S[i]
        return result
    ```
      * 단위 연산 : 리스트의 원소를 result에 더하는 명령
      * 입력 크기 : 리스트 S의 원소 개수(n)
      * for 문장은 항상 n번 실행하므로 다음과 같이 표현
        * 시간복잡도: T(n) = n
  <br><br>

  * Algorithm 1.3 (교환 정렬)의 시간 복잡도 분석
    ```python
      def exchange(S):
        n = len(S)
        for i in ragne(n - 1):
            if (S[i] > S[j]):
              S[i], S[j] = S[j], S[i] # swap
    ```
      * 단위 연산 : S[i]와 S[j]의 비교 (swap은 if연산에 따라 실행하기도 안하기도 하므로 비교연산을 지정)
      * 입력 크기 : 정렬할 리스트 S의 원소 개수(n)
      * for-j 루프는 i에 따라 n-1번에서 1번까지 실행하므로 다음과 같이 계산
        * 시간복잡도: T(n) = (n-1) + (n-2) + ... + 1 = (n-1)n/2
  <br><br>

  * Algorithm 1.4 (행렬 곱셈)의 시간 복잡도 분석
    ```python
      def matrixMult(n, A, B):
       C = [[0] * n for _ in range(n)]
       for i in ragne(n):
        for j in ragne(n):
          for k in ragne(n):
            C[i][j] += A[i][k] * B[k][j]
        return C
    ```
      * 단위 연산 : 가장 안쪽 for 루프에 있는 곱셈 연산(덧셈도 가능 하지만 곱셈이 더 무게가 큼)
      * 입력 크기 : 행과 열의 개수(n)
      * 3중 for루프가 항상 n번 실행하므로 다음과 같이 계산
        * 시간복잡도: T(n) = n x n x n = n^3
  <br><br>
* 단위 연산의 실행 횟수는 항상 일정한가?
  * Algorithm 1.2, 1.3, 1.4의 경우 : 항상 일정
  * Algorithm 1.1(순차탐색)의 경우 : 입력 사례에 따라 다름
    * S = [4, 5, 6, 7, 3, 8, 9], x = 3 : 5번 실행
    * S = [4, 5, 6, 7, 3, 8, 9], x = 5 : 2번 실행
    * S = [4, 5, 6, 7, 3, 8, 9], x = 9 : 7번 실행

* 입력 사례에 따른 시간 복잡도 분석
  * 일정 시간 복잡도 : 입력 사례에 따라 달라지지 않는 경우 (1.2, 1.3, 1.4)
  * 최악, 최적, 평균 시간 복잡도 분석 : 입력 사례에 따라 달라지는 경우
    * 알고리즘의 효율성을 따질때는 `최악의 시간 복잡도 분석`이 중요한 경우 많음
<br><br>
* Algorithm 1.1(순차 탐색)의 시간 복잡도 분석
  * 단위 연산 : 리스트의 원소와 주어진 키 x와의 비교 연산
  * 입력 크기 : 리스트 원소의 개수(n)
  * 최악의 경우는 모두 비교 : W(n) = n
  * 최적의 경우는 한 번만 비교 : B(n) = 1
  * 평균의 경우 : 주어진 키 x가 k번째에 있으면 k번을 비교함
    * 만약 어떤 키 x는 리스트 S에 골고루 분포해 있다고 한다면,
    * ![avg](https://user-images.githubusercontent.com/66772624/155292391-3170bfa9-2c95-419b-bf06-0e9c26976b4a.png)
<br><br>

### [1.3 알고리즘의 차수]
<br>

* 어떤 알고리즘이 (궁극적으로) 더 빠른가?
  * 시간 복잡도 : 입력 크기(n)에 대한 단위 연산 횟수의 함수 f(n)을 두었을 떄
  * 시간 복잡도가 f1(n) = n인 알고리즘과 f2(n) = n^2인 알고리즘을 보면 f2가 f1보다 크다는걸 쉽게 알 수 있다.
  * 만약 단위 연산의 실행 시간이 f2는 t이고, f1은 1000t일 경우
    * f1의 단위 연산이 f2의 단위 연산보다 1000배 느리지만
    * 알고리즘의 전체 실행 시간은 f1이 n x 1000t, f2는 n^2 x t 이므로
    * 부등식 n^2 x t > n x 1000t이 성립하려면
    * n > 1000
    * 즉, n이 1000보다 크면 f1이 f2보다 궁극적으로 더 빠르다고 할 수 있다.
<br><br>
* 차수(Order): `알고리즘의 궁극적인 성능 분류의 척도`(제일 높은 차수)
  * 1차 시간 알고리즘 : 시간 복잡도가 1차 함수인 알고리즘
  * 2차 시간 알고리즘 : 시간 복잡도가 2차 함수인 알고리즘
  * 근본 원리 : 모든 1차 시간 알고리즘은 궁극적으로 2차 시간 알고리즘보다 빠르다.
  * 따라서, `시간 복잡도 함수의 차수`로 `알고리즘의 성능`을 `분류`할 수 있다.
<br><br>
* 자주 사용되는 복잡도 분류
  * ![complexity](https://user-images.githubusercontent.com/66772624/155300229-04febd42-49c7-462d-935e-4a2a3ce94cdd.png)
<br><br>
* 알고리즘의 성능을 차수로 분류하는 법
  * 복잡도 함수를 분류할 때 낮은 차수의 항들은 항상 버릴 수 있다.
  * 예를 들어, an^2 + bn + c의 함수를 2차 시간 함수(n^2)로 분류
  * (궁극적으로) 2차 항이 이 함수의 값을 결정하는 데 가장 중요하기 때문
* 점근적 표기법: 빅-오 표기법 O(n^2)
  * 빅오(O): 복잡도 함수의 점근적 상한을 표기 (적어도 이것을 넘어서진 않는다는 의미)
  * 오메가(Ω): 복잡도 함수의 점근적 하한을 표기 (적어도 이 밑으로 떨어지진 않는다는 의미)
    * 오메가는 언제 사용될까? : 어떤 문제의 하한을 구할때 (제일 좋은 효율)
  * 쎼타(θ)=차수(진정한 Order) : 복잡도 함수의 점근적 상한과 하한을 동시에 만족
<br><br>
* Algorithm 1.3 (교환 정렬)의 차수
  * T(n) = (n-1)n / 2 = 1/2n^2 - 1/2n
  * O(n^2), Ω(n^2), 따라서 θ(n^2)이 된다.
  * 그러므로 교환 정렬 알고리즘의 차수는 θ(n^2) 
<br><br>
* Algorithm 1.1 (순차 탑색)의 차수
  * 최악의 경우 : W(n) = n 은 θ(n) 
  * 최적의 경우 : B(n) = 1 은 θ(1) 
  * 평균의 경우 : A(n) = (n+1)/2 은 θ(n) 

<br><br>
### [2.3 분할정복 설계 방법]

* 분할정복 설계 전략
  * 분할 : 문제의 입력사례를 둘 이상의 작은 입력사례로 분할
  * 정복 : 작은 입력사례들을 각각 정복, 충분히 작지 않다면 재귀호출
  * 통합 : 필요하면, 작은 입력사례의 해답을 통합하여 원래 입력사례의 해답을 도출

* 알고리즘을 푸는 방법
  * Brute Force(단순무식) : Sequential Search(순차검색)
  * Div & Conq(분할정복) : 이진탐색, 합병정렬
  * Greedy(탐욕법) : 여러 경우 중 하나를 고를때 순간에 최적이라고 생각되는 것을 선택해 나가는 방식
  * Dynamic Programming(동적계획법) : Div & Conq가 top-down이면 동적계획법은 bottom-up이다. 복잡한 문제를 간단한 여러 개의 문제로 나누어 푸는 방법

* 분할정복 알고리즘
  * 분할정복 vs 동적계획법
    * 하향식(Top-Down) vs 상향식(Bottom-Up)문제풀이 방식
      > 피보나치 수열을 재귀호출을 이용함 (Top-Down)    
      > 피보나치 수열을 차례대로 구하는것(리스트를 이용) (Bottom-Up)

  * 분할정복 vs 탐욕법
    * 탐욕법은 가장 비효율적인 분할정복 알고리즘?
      > 교환정렬에서 하나씩 찾아가는것(탐욕법)    
       교환정렬도 따지고 보면 1개와 나머지들을 나누는 것이므로 가장 비효율적인 분할정복 알고리즘이라고 설명할 수 있다.

<br><br>
### [2.4 퀵 정렬(분할 교환 정렬)]
* 대표적인 분할정복 알고리즘
* 대표적인 `내부(in-place)정렬` : 추가적인 리스트를 사용하지 않는 정렬(합병정렬의 경우 외부 리스트를 사용해야함)

* Quick-Sort
  * divide: 기준 원소(pivot)를 정해서 기준원소를 기준으로 좌우로 분할
  * conquer: 왼쪽의 리스트와 오른쪽의 리스트를 각각 재귀적으로 퀵 정렬
  * obtain: 정렬된 리스트를 리턴
<br><br>
  * 기준 원소(pivot)는 어떻게 정할까?
    * 편의상, 일단 리스트의 첫 원소를 기준원소로 정하도록 하자
  * 기준 원소로 어떻게 리스트를 나눌 수 있을까?
    * 두개의 인덱스(i, j)를 이용해서 비교(compare)와 교환(swap)   
    (partition알고리즘은 퀵소트 뿐아니라 여러가지 알고리즘에서 많이 사용된다)

* 분할 정복이 가지고 있는 개념을 알아봄 (Divide, Conquer)
  * 대표적인 알고리즘인 퀵 정렬을 알아봄 : divide를 pivot을 가지고 함
    > pivot을 원소의 첫번째 아이템으로 정하므로 최악의 경우에는 순차적으로 재귀반복함
    > 퀵정렬은 정렬이 조금 되어있는 리스트의 경우 성능이 떨어진다. 오히려 랜덤하게 섞여있어야 성능 
    
<br><br>
### [2.5 쉬트라쏀의 행렬 곱셈]
* 행렬 곱셈 문제
  * 문제: 두 n X n 행렬의 곱을 구하시오
  * Algorithm 1.4 : 행렬 곱셈의 정의에 충실 시간복잡도는 O(n^3)
  * Algorithm 2.8 : 쉬트라센의 방법을 사용 시간복잡도는 O(n^2.81)

* 쉬트라센의 행렬은 7번의 곱셈과 18번의 더하기 및 빼기가 존재   
  (기존의 행렬곱은 8번의 곱셈과 4번의 덧셈이므로 부담이 큰 곱셈이 1 감소함)
* 쉬트라센의 방법: Divide and Conquer
  * 2x2의 행렬을 꼭 맞추지 않고 큰 행렬을 4개의 부분 행렬로 나누어서 정복하면 됨